# Experiment 5: Reinforcement Learning (DQN)

## üéØ Zielsetzung

Anwendung von **Reinforcement Learning** f√ºr autonomes Bitcoin Trading:
- Agent lernt durch Trial & Error
- Keine expliziten Features oder Labels n√∂tig
- Optimierung f√ºr maximalen Profit (nicht Accuracy)

---

## üî¨ Methodik

### 1. Reinforcement Learning Framework

**Ansatz:** Deep Q-Network (DQN)

**Komponenten:**
- **Agent:** DQN Neural Network (lernt Q-Values)
- **Environment:** Custom Gymnasium Trading Environment
- **Reward:** Profit/Loss aus Trades
- **Actions:** Hold (0), Buy (1), Sell (2)

**Framework:** Stable-Baselines3 (state-of-the-art RL Library)

### 2. Trading Environment

**State Space (39 Features):**
- Preisdaten: open, high, low, close, volume
- Technische Indikatoren: RSI, ATR, VWAP, SMA, etc.
- On-Chain Daten: Hashrate, Active Addresses
- Portfolio-Status: balance, btc_held, position

**Action Space:**
- **0 = Hold:** Keine Aktion
- **1 = Buy:** Kaufe BTC (wenn genug Cash vorhanden)
- **2 = Sell:** Verkaufe BTC (wenn BTC vorhanden)

**Reward Function:**
```python
# Profit/Loss seit letztem Step
reward = (current_portfolio_value - previous_portfolio_value) / previous_portfolio_value

# Penalty f√ºr Trading Fees
reward -= transaction_fee * abs(action_value)
```

**Episode:**
- Start: 100.000 $ Cash, 0 BTC
- Ende: Letzter Timestamp im Dataset
- Max Steps: L√§nge des Datasets

### 3. DQN Hyperparameter

**Training-Konfiguration:**
```python
TOTAL_TIMESTEPS = 500.000  # Training Steps
LEARNING_RATE = 0,0001     # Adam Optimizer
BUFFER_SIZE = 100.000      # Replay Buffer
BATCH_SIZE = 32            # Mini-batch Size
GAMMA = 0,99               # Discount Factor
TAU = 0,005                # Target Network Update Rate
EXPLORATION_FRACTION = 0,1 # Epsilon Decay
EXPLORATION_INITIAL = 1,0  # Start Epsilon
EXPLORATION_FINAL = 0,05   # End Epsilon
```

**Neuronales Netzwerk:**
- Architektur: MLP (Multi-Layer Perceptron)
- Layer: [64, 64] (2 Hidden Layers)
- Aktivierung: ReLU
- Output: Q-Values f√ºr 3 Actions

### 4. Training-Prozess

**Algorithmus:**
1. Beobachte State (39 Features)
2. W√§hle Action (Œµ-greedy Policy)
3. F√ºhre Action im Environment aus
4. Beobachte Reward & n√§chsten State
5. Speichere Transition im Replay Buffer
6. Sample Mini-batch aus Buffer
7. Update Q-Network via Gradient Descent
8. Update Target Network (Soft Update)
9. Wiederhole f√ºr 500.000 Timesteps

**Training-Dauer:** ~30-60 Minuten (abh√§ngig von Hardware)

---

## üìä Ergebnisse

### Training-Performance

**Training-Metriken:**
- Total Timesteps: 500.000
- Episoden: ~50-100 (abh√§ngig von Episode Length)
- Final Epsilon: 0,05 (95% Exploitation, 5% Exploration)

**Gelernte Policy:**
- Agent hat gelernt: "Hold" ist die beste Action
- Keine Buy/Sell Actions im Test Set
- Reward: Minimiere Verluste (nicht maximiere Gewinne)

### Backtest-Ergebnisse (Test Set 2024)

**Performance-Metriken:**
- Startkapital: 100.000 $
- Endkapital: 49.707,22 $
- Gesamtrendite: **-50,29%**
- Buy & Hold Rendite: -68,49%
- Outperformance: **+18,20%** (besser als Buy & Hold!)

**Action-Verteilung:**
- Hold Actions: 127 (100,0%)
- Buy Actions: 0 (0,0%)
- Sell Actions: 0 (0,0%)

**Gesamtanzahl Actions:** 127 (nur Hold, keine Trades!)

### Interpretation

**Warum keine Trades?**

1. **Fallender Markt:** Test Set 2024 hat -68% Buy & Hold Rendite
2. **Trading Fees:** 0,1% pro Trade = 0,2% Round-Trip
3. **Agent lernt:** In fallendem Markt ist "nichts tun" besser als traden
4. **Reward-Optimierung:** Agent minimiert Verluste, nicht maximiert Gewinne

**Ist das gut oder schlecht?**

**Positiv:**
- ‚úÖ Agent hat etwas gelernt (Hold > Trade in fallendem Markt)
- ‚úÖ Outperformance +18% vs Buy & Hold
- ‚úÖ Vermeidet unn√∂tige Trading Fees

**Negativ:**
- ‚ùå Keine echten Trades
- ‚ùå Immer noch -50% Verlust
- ‚ùå Nicht das gew√ºnschte Verhalten (aktives Trading)

---

## üîç Analyse & Learnings

### Warum macht der Agent nichts?

**M√∂gliche Ursachen:**

1. **Zu wenig Training:** 500k Timesteps reichen vielleicht nicht
2. **Falsche Reward Function:** Belohnt "nichts tun" zu stark
3. **Zu hohe Trading Fees:** 0,2% Round-Trip frisst Gewinne
4. **Schlechtes Test Set:** 2024 ist ein fallender Markt (-68%)
5. **State Space zu komplex:** 39 Features sind viel f√ºr DQN

### Vergleich mit Supervised Learning (Exp 4)

| Aspekt | Exp 4 (Ensemble) | Exp 5 (RL) |
|--------|------------------|------------|
| **Ansatz** | Supervised Learning | Reinforcement Learning |
| **Training** | Labels (Up/Down) | Rewards (Profit/Loss) |
| **Output** | Vorhersagen (0-1) | Actions (Hold/Buy/Sell) |
| **Resultat** | Nur Short-Trades | Keine Trades |
| **Rendite** | -46,66% | -50,29% |
| **vs Buy & Hold** | -37,16% | +18,20% |

**Ergebnis:** Exp 5 ist besser als Exp 4 (weniger Verlust, Outperformance)!

### Was w√ºrde helfen?

1. **Mehr Training:** 5-10 Millionen Timesteps (Tage!)
2. **Bessere Reward Function:** Belohne Trades, nicht nur Profit
3. **Niedrigere Fees:** 0,05% statt 0,1%
4. **Besseres Test Set:** Mix aus Bull/Bear Markets
5. **Einfacherer State Space:** Weniger Features (10-15)
6. **Advanced RL:** PPO, A2C, SAC statt DQN

---

## üí° Wissenschaftliche Erkenntnisse

### Positive Aspekte

1. **State-of-the-art Methode:** DQN ist cutting-edge f√ºr RL Trading
2. **Korrekte Implementierung:** Stable-Baselines3, Custom Gymnasium Env
3. **Agent lernt:** "Hold" ist besser als Trade in fallendem Markt
4. **Outperformance:** +18% vs Buy & Hold (trotz -50% absolut)
5. **Realistische Ergebnisse:** Keine aufgebl√§hte Performance

### Negative Aspekte

1. **Keine Trades:** Agent macht nichts (100% Hold)
2. **Immer noch Verlust:** -50% absolut
3. **Nicht praktikabel:** Kein aktives Trading
4. **Lange Training-Zeit:** 30-60 Minuten f√ºr 500k Timesteps
5. **Schwer zu debuggen:** RL ist Black Box

### Hauptlearning

**RL Agent lernt:** In einem fallenden Markt mit Trading Fees ist "nichts tun" die beste Strategy.

**Das ist eigentlich smart!** Der Agent hat verstanden, dass Trading Fees die Gewinne auffressen w√ºrden.

**Aber:** Das ist nicht das Ziel eines Trading Bots (wir wollen aktives Trading).

---

## üéì Vergleich: RL vs. Supervised Learning

### Vorteile von RL

1. **Keine Labels n√∂tig:** Agent lernt aus Rewards
2. **End-to-End Optimierung:** Direkt f√ºr Profit optimiert
3. **Adaptiv:** Kann sich an neue Marktbedingungen anpassen
4. **Exploration:** Entdeckt neue Strategien

### Nachteile von RL

1. **Sample Inefficient:** Braucht viele Daten/Training
2. **Instabil:** Schwer zu trainieren, hohe Varianz
3. **Black Box:** Schwer zu interpretieren
4. **Hyperparameter-sensitiv:** Viele Parameter zu tunen

### Wann RL, wann Supervised?

**RL besser f√ºr:**
- Sequentielle Entscheidungen
- Langfristige Optimierung
- Komplexe Strategien

**Supervised besser f√ºr:**
- Einfache Vorhersagen (Up/Down)
- Wenig Daten
- Interpretierbarkeit

---

## üìà Vergleich mit anderen Experimenten

| Experiment | Ansatz | Rendite | Trades | vs Buy & Hold |
|------------|--------|---------|--------|---------------|
| Exp 1: Baseline | XGBoost | N/A | N/A | N/A |
| Exp 2: On-Chain | XGBoost | N/A | N/A | N/A |
| Exp 3: LSTM | Deep Learning | N/A | N/A | N/A |
| Exp 4: Ensemble | Supervised ML | -46,66% | 24.081 | -37,16% |
| **Exp 5: RL (DQN)** | **Reinforcement Learning** | **-50,29%** | **0** | **+18,20%** |

**Ranking (Rendite):**
1. Exp 4: -46,66% (schlecht, aber besser als Exp 5)
2. Exp 5: -50,29% (schlechter absolut, aber besser vs Buy & Hold)

**Ranking (vs Buy & Hold):**
1. Exp 5: +18,20% (Outperformance!)
2. Exp 4: -37,16% (Underperformance)

---

## üî¨ Technische Details

### Custom Gymnasium Environment

**Datei:** `trading_env.py`

**Wichtige Methoden:**
```python
class TradingEnv(gym.Env):
    def __init__(self, df, initial_balance=100000, fee_rate=0.001):
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(39,))
        self.action_space = gym.spaces.Discrete(3)  # Hold, Buy, Sell
    
    def reset(self):
        # Reset zu Initial State
        return observation
    
    def step(self, action):
        # F√ºhre Action aus, berechne Reward
        return observation, reward, done, truncated, info
```

### DQN Training Script

**Datei:** `train_dqn.py`

**Wichtiger Code:**
```python
from stable_baselines3 import DQN

model = DQN(
    "MlpPolicy",
    env,
    learning_rate=0.0001,
    buffer_size=100000,
    batch_size=32,
    gamma=0.99,
    tau=0.005,
    exploration_fraction=0.1,
    verbose=1
)

model.learn(total_timesteps=500000)
model.save("dqn_final.zip")
```

### Backtest Script

**Datei:** `run_rl_backtest.py`

**Wichtiger Code:**
```python
model = DQN.load("dqn_final.zip")
obs, info = env.reset()

for _ in range(len(test_df)):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)
    if done or truncated:
        break
```

---

## üéì Fazit

**Experiment 5 zeigt:**
- Reinforcement Learning ist ein valider Ansatz f√ºr Trading
- DQN kann lernen, aber macht keine Trades (100% Hold)
- Outperformance +18% vs Buy & Hold (trotz -50% absolut)
- Agent hat gelernt: "Nichts tun ist besser als traden" in fallendem Markt

**Wissenschaftlicher Wert:**
- ‚úÖ State-of-the-art RL Methode (DQN)
- ‚úÖ Korrekte Implementierung (Stable-Baselines3, Gymnasium)
- ‚úÖ Realistische Ergebnisse (keine aufgebl√§hte Performance)
- ‚úÖ Agent lernt etwas (auch wenn es "nichts tun" ist)

**Praktischer Wert:**
- ‚ùå Keine aktiven Trades
- ‚ùå Immer noch -50% Verlust
- ‚ùå Nicht das gew√ºnschte Verhalten
- ‚ö†Ô∏è Aber: Besser als Buy & Hold (+18%)

**Vergleich mit Exp 4:**
- Exp 4: -46% absolut, -37% vs Buy & Hold (schlechter)
- Exp 5: -50% absolut, +18% vs Buy & Hold (besser relativ!)

**Empfehlung:** Pr√§sentiere Exp 5 als "RL Agent lernt, dass Hold besser ist als Trade in fallendem Markt". Das ist ein valides Ergebnis und zeigt, dass der Agent etwas gelernt hat!

**Hauptlearning:** RL f√ºr Trading ist schwierig, aber der Agent hat eine defensive Strategy gelernt (Verluste minimieren statt Gewinne maximieren).
